{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7958771d-471f-4754-bce7-7fb9eb04400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "s0_default: float = 1\n",
    "p_default: float = 0.5\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000\n",
    "\n",
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        self.w -= gradient * mu_default*(s0_default/(s0_default + iteration))**p_default\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        dh = 1e-4\n",
    "        dw = np.eye(self.w.shape[0]) * dh\n",
    "        M = np.vstack([self.w for i in range(self.w.shape[0])]) + dw\n",
    "        M = M.transpose()\n",
    "        M = np.zeros((self.w.shape[0], self.w.shape[0])) + self.w[:, np.newaxis] + dw\n",
    "        gradient = (np.linalg.norm((X@M).transpose() - y, axis=1) - np.linalg.norm(X@self.w - y))/dh\n",
    "        return gradient\n",
    "        \n",
    "\n",
    "\n",
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        self.w -= gradient * self.eta(iteration)\n",
    "    \n",
    "class MomentumDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Momentum gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, alpha: float = alpha_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param alpha: momentum coefficient\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.alpha = alpha\n",
    "        self.w = np.copy(w0)\n",
    "        self.h = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        self.h = self.alpha * self.h + self.eta(iteration) * gradient\n",
    "        self.w -= self.h\n",
    "\n",
    "class Adagrad(BaseDescent):\n",
    "    \"\"\"\n",
    "    Adaptive gradient algorithm class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, eps: float = eps_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param eps: smoothing term (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.eps = eps\n",
    "        self.w = np.copy(w0)\n",
    "        self.g = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        self.g = self.g + gradient**2\n",
    "        self.w -= self.eta(iteration) * gradient / (self.eps + self.g)**0.5\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "de8fad46-1c2b-4faf-abef-b34124cd3cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55564467, 0.97336739])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_objects = 5\n",
    "dimension = 2\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = 1 * X[:, 0] + 2 * X[:, 1]\n",
    "w0 = np.zeros(dimension) \n",
    "gg = GradientDescent(w0, 0.01)\n",
    "n_steps = 1000\n",
    "for i in range(n_steps):\n",
    "    gg.step(X, y, i)\n",
    "gg.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ae8cc21f-9711-49b2-a525-aa23423378b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.02139936, 2.03684673])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg = MomentumDescent(w0, 1, 0.01)\n",
    "for i in range(n_steps):\n",
    "    gg.step(X, y, i)\n",
    "gg.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c3b893f9-27e1-452c-8e8a-3fc7f242ef54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99751261, 1.99752307])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg = Adagrad(w0, 10)\n",
    "for i in range(n_steps):\n",
    "    gg.step(X, y, i)\n",
    "gg.w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
